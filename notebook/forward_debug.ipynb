{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "os.chdir('..')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from model.VAE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  780.484130859375\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "batch_size = 5\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Annealing parameter\n",
    "k = 0.0025\n",
    "x0 = 2500\n",
    "\n",
    "loss_function = torch.nn.NLLLoss(reduction='sum')\n",
    "\n",
    "model = VAE().to(device)\n",
    "\n",
    "chord_onehot = torch.randn(batch_size, 272, 96)\n",
    "length = torch.Tensor([272, 100, 200, 150, 120]).long()\n",
    "\n",
    "chord_pred, logp, mu, log_var, input_x = model(chord_onehot,length)\n",
    "\n",
    "# flatten tensor to calculate loss\n",
    "# chord = torch.empty(batch_size, 272, 96 + 1, dtype=torch.long).random_(0, 1)\n",
    "\n",
    "# Arrange \n",
    "chord_flatten = []\n",
    "logp_flatten = []\n",
    "length = length.squeeze()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Get predicted softmax chords by length of the song (cutting off padding 0), (1,length,96)\n",
    "    logp_flatten.append(logp[i][:length[i]])\n",
    "\n",
    "    # Get groundtruth chords by length of the song (cutting off padding 0), (1,length)\n",
    "    chord_flatten.append(chord_onehot[i][:length[i]])\n",
    "\n",
    "# Rearrange for loss calculation\n",
    "logp_flatten = torch.cat(logp_flatten, dim=0)\n",
    "chord_flatten = torch.cat(chord_flatten,dim=0)\n",
    "chord_groundtruth_index = torch.max(chord_flatten,1).indices\n",
    "\n",
    "# loss calculation\n",
    "# Add weight to NLL also\n",
    "NLL_loss, KL_loss, KL_weight = loss_fn(loss_function = loss_function, logp = logp_flatten, target = chord_groundtruth_index, length = length, mean = mu, log_var = log_var,anneal_function='logistic', step=step, k=k, x0=x0)\n",
    "# NLL_loss, KL_loss, KL_weight = loss_fn(logp = chord_pred_flatten, target = chord_groundtruth_index, length = length, mean = mu, log_var = log_var,anneal_function='logistic', step=step, k=k, x0=x0)\n",
    "\n",
    "loss = (NLL_loss + KL_weight * KL_loss) / batch_size\n",
    "print('loss: ',loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from model.Parameter_CVAE import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 272, 16])\n",
      "loss:  1780.773681640625\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "batch_size = 5\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "# Annealing parameter\n",
    "k = 0.0025\n",
    "x0 = 2500\n",
    "\n",
    "loss_function = torch.nn.NLLLoss(reduction='sum')\n",
    "\n",
    "model = CVAE().to(device)\n",
    "\n",
    "melody = torch.randn(batch_size, 272, 12 * 24 * 2)\n",
    "chord_onehot = torch.randn(batch_size, 272, 96)\n",
    "length = torch.Tensor([272, 100, 200, 150, 120]).long()\n",
    "r_pitch = torch.Tensor([272, 100, 200, 150, 120]).view(batch_size,1,1).expand(batch_size,272,1)\n",
    "r_rhythm = torch.Tensor([272, 100, 200, 150, 120]).view(batch_size,1,1).expand(batch_size,272,1)\n",
    "\n",
    "z = torch.randn(batch_size,272,16)\n",
    "z = torch.cat((z, melody, r_pitch, r_rhythm), dim=-1)\n",
    "\n",
    "chord_pred,logp ,mu, log_var, input_x = model(chord_onehot,melody,length,r_pitch,r_rhythm)\n",
    "\n",
    "# flatten tensor to calculate loss\n",
    "# chord = torch.empty(batch_size, 272, 96 + 1, dtype=torch.long).random_(0, 1)\n",
    "\n",
    "# Arrange \n",
    "chord_flatten = []\n",
    "logp_flatten = []\n",
    "length = length.squeeze()\n",
    "\n",
    "for i in range(batch_size):\n",
    "    # Get predicted softmax chords by length of the song (cutting off padding 0), (1,length,96)\n",
    "    logp_flatten.append(logp[i][:length[i]])\n",
    "\n",
    "    # Get groundtruth chords by length of the song (cutting off padding 0), (1,length)\n",
    "    chord_flatten.append(chord_onehot[i][:length[i]])\n",
    "\n",
    "# Rearrange for loss calculation\n",
    "logp_flatten = torch.cat(logp_flatten, dim=0)\n",
    "chord_flatten = torch.cat(chord_flatten,dim=0)\n",
    "chord_groundtruth_index = torch.max(chord_flatten,1).indices\n",
    "\n",
    "# loss calculation\n",
    "# Add weight to NLL also\n",
    "NLL_loss, KL_loss, KL_weight = loss_fn(loss_function = loss_function, logp = logp_flatten, target = chord_groundtruth_index, length = length, mean = mu, log_var = log_var,anneal_function='logistic', step=step, k=k, x0=x0)\n",
    "# NLL_loss, KL_loss, KL_weight = loss_fn(logp = chord_pred_flatten, target = chord_groundtruth_index, length = length, mean = mu, log_var = log_var,anneal_function='logistic', step=step, k=k, x0=x0)\n",
    "\n",
    "loss = (NLL_loss + KL_weight * KL_loss) / batch_size\n",
    "print('loss: ',loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(batch_size,272,16)\n",
    "z = torch.cat((z, melody,r_pitch), dim=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 272, 593])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 272, 576])\n"
     ]
    }
   ],
   "source": [
    "print(melody.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 272, 592])\n"
     ]
    }
   ],
   "source": [
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 272, 1])\n"
     ]
    }
   ],
   "source": [
    "print(r_pitch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.7862, -0.5978, -0.0300,  0.8419, -1.1588])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7862]],\n",
       "\n",
       "        [[-0.5978]],\n",
       "\n",
       "        [[-0.0300]],\n",
       "\n",
       "        [[ 0.8419]],\n",
       "\n",
       "        [[-1.1588]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = a.view(batch_size,1,1)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 272, 1])\n"
     ]
    }
   ],
   "source": [
    "c = b.expand(5, 272, 1)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
